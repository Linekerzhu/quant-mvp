# å¤–éƒ¨å®¡è®¡é˜»å¡é¡¹æ•´æ”¹è®¡åˆ’

**ç¼–åˆ¶äºº**: å¼ å¾—åŠŸï¼ˆå…«å“é¢†ä¾ï¼‰  
**ç¼–åˆ¶æ—¥æœŸ**: 2026-02-28  
**æ•´æ”¹ç›®æ ‡**: ä¿®å¤3ä¸ªCRITICALé˜»å¡é¡¹ï¼Œç¡®ä¿Phase Då¯è¿›å…¥

---

## ä¸€ã€æ•´æ”¹æ¦‚è§ˆ

| ç¼–å· | é—®é¢˜ | ä¸¥é‡çº§åˆ« | è´Ÿè´£äºº | é¢„è®¡å·¥æœŸ | éªŒæ”¶çŠ¶æ€ |
|------|------|----------|--------|----------|----------|
| C-01 | PBOè®¡ç®—é€»è¾‘å®Œå…¨æ— æ•ˆ | CRITICAL | æå¾—å‹¤ | 4h | â³ å¾…ä¿®å¤ |
| C-02 | MetaTrainerç«¯åˆ°ç«¯ä¸å¯è¿è¡Œ | CRITICAL | æå¾—å‹¤ | 8h | â³ å¾…ä¿®å¤ |
| C-03 | Sample Weightsæœªä¼ å…¥LightGBM | CRITICAL | æå¾—å‹¤ | 2h | â³ å¾…ä¿®å¤ |
| H-01 | DSRæ•°å€¼ä¸ç¨³å®š | HIGH | æå¾—å‹¤ | 2h | â³ å¾…ä¿®å¤ |
| H-02 | CPCVä½¿ç”¨æ—¥å†æ—¥è€Œéäº¤æ˜“æ—¥ | HIGH | æå¾—å‹¤ | 3h | â³ å¾…ä¿®å¤ |
| H-03 | BDay(1)è½å‡æ—¥é—®é¢˜ | HIGH | æå¾—å‹¤ | 2h | â³ å¾…ä¿®å¤ |
| H-04 | backtest/executionç©ºå£³ | HIGH | æå¾—å‹¤ | åç»­Phase | â³ å¾…è§„åˆ’ |
| H-05 | assertå¯ç»•è¿‡ | HIGH | æå¾—å‹¤ | 1h | â³ å¾…ä¿®å¤ |

**æ€»å·¥æœŸä¼°ç®—**: 22å°æ—¶ï¼ˆçº¦3ä¸ªå·¥ä½œæ—¥ï¼‰

---

## äºŒã€CRITICALé˜»å¡é¡¹æ•´æ”¹æ–¹æ¡ˆ

### ğŸ”´ C-01: PBOè®¡ç®—é€»è¾‘å®Œå…¨æ— æ•ˆ

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/overfitting.py` ç¬¬57-65è¡Œ
- **ç—‡çŠ¶**: `np.argsort(np.argsort(aucs))` äº§ç”Ÿ [0,1,2,...,14] æ’å®šåºåˆ—
- **å½±å“**: PBOæ’ç­‰äº0.533ï¼Œæ— æ³•çœŸå®åæ˜ è¿‡æ‹Ÿåˆç¨‹åº¦
- **æ ¹æœ¬åŸå› **: åŒé‡argsortäº§ç”Ÿæ’ååºåˆ—ï¼Œè€Œéæ¯”è¾ƒIS vs OOSæ€§èƒ½

**é—®é¢˜ä»£ç **:
```python
def calculate_pbo(self, path_results: List[Dict]) -> float:
    aucs = [r['auc'] for r in path_results]
    n = len(aucs)
    
    # âŒ é”™è¯¯ï¼šåŒé‡argsortäº§ç”Ÿ [0,1,2,...,n-1]
    ranked = np.argsort(np.argsort(aucs))
    
    # âŒ é”™è¯¯ï¼šè¿™æ°¸è¿œè¿”å›çº¦ 0.5
    pbo = np.mean(ranked < n / 2)
    
    return float(pbo)
```

**æ•´æ”¹æ–¹æ¡ˆ**:

**æ–¹æ¡ˆA: å®ç°çœŸæ­£çš„PBOï¼ˆæ¨èï¼‰**

æŒ‰ç…§ Bailey & LÃ³pez de Prado (2017) çš„å®šä¹‰ï¼š

```python
def calculate_pbo(self, path_results: List[Dict]) -> float:
    """
    è®¡ç®— PBOï¼ˆProbability of Backtest Overfittingï¼‰ã€‚
    
    åŸºäº Bailey & LÃ³pez de Prado (2017):
    PBO = Prob(rank_IS != rank_OOS_max)
    
    å¯¹äºæ¯ä¸ªç»„åˆçš„æµ‹è¯•é›†ï¼Œè®¡ç®—ï¼š
    1. IS (in-sample) AUCæ’å
    2. OOS (out-of-sample) AUC
    3. æ¯”è¾ƒæœ€ä¼˜ISæ¨¡å‹åœ¨OOSä¸Šçš„è¡¨ç°
    """
    n = len(path_results)
    if n == 0:
        return 1.0
    
    # æå– IS å’Œ OOS AUC
    is_aucs = [r.get('is_auc', r['auc']) for r in path_results]
    oos_aucs = [r.get('oos_auc', r['auc']) for r in path_results]
    
    # æ‰¾åˆ° IS è¡¨ç°æœ€å¥½çš„è·¯å¾„
    is_ranking = np.argsort(is_aucs)[::-1]  # é™åºï¼Œbest first
    best_is_idx = is_ranking[0]
    
    # è®¡ç®— OOS æ’å
    oos_ranking = np.argsort(np.argsort(oos_aucs)[::-1])  # é™åºæ’å
    best_is_oos_rank = oos_ranking[best_is_idx]
    
    # PBO: æœ€ä¼˜ISæ¨¡å‹åœ¨OOSä¸­æ’åé åçš„æ¦‚ç‡
    # å¦‚æœæ’ååœ¨ä¸‹åŠéƒ¨åˆ†ï¼ˆæ’å >= n/2ï¼‰ï¼Œè§†ä¸ºè¿‡æ‹Ÿåˆ
    pbo = 1.0 if best_is_oos_rank >= n / 2 else 0.0
    
    return float(pbo)
```

**æ–¹æ¡ˆB: ä¿å®ˆä¼°è®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰**

å¦‚æœæš‚æ—¶æ— æ³•è·å–IS/OOSåˆ†ç¦»æ•°æ®ï¼š

```python
def calculate_pbo(self, path_results: List[Dict]) -> float:
    """
    ä¿å®ˆä¼°è®¡ï¼šè®¡ç®—æµ‹è¯•é›†AUCçš„æ–¹å·®ç³»æ•°ã€‚
    
    é«˜æ–¹å·® = é«˜è¿‡æ‹Ÿåˆé£é™©
    CV = std / mean
    """
    aucs = [r['auc'] for r in path_results]
    n = len(aucs)
    
    if n == 0:
        return 1.0
    
    mean_auc = np.mean(aucs)
    std_auc = np.std(aucs, ddof=1)
    
    # å˜å¼‚ç³»æ•°
    cv = std_auc / mean_auc if mean_auc > 0 else 1.0
    
    # æ˜ å°„åˆ° [0, 1] åŒºé—´
    # CV < 0.1 -> PBO â‰ˆ 0
    # CV > 0.3 -> PBO â‰ˆ 1
    pbo = min(1.0, max(0.0, (cv - 0.05) / 0.25))
    
    return float(pbo)
```

**æ¨è**: æ–¹æ¡ˆAï¼ˆç¬¦åˆAFMLå®šä¹‰ï¼‰

**éªŒæ”¶æ ‡å‡†**:
1. âœ… PBOå€¼ä¸å†æ’ç­‰äº0.533
2. âœ… ä¸åŒæ•°æ®é›†äº§ç”Ÿä¸åŒPBOå€¼
3. âœ… å•å…ƒæµ‹è¯•éªŒè¯é€»è¾‘æ­£ç¡®æ€§
4. âœ… æ–‡æ¡£æ›´æ–°ï¼Œè¯´æ˜PBOè®¡ç®—æ–¹æ³•

**æµ‹è¯•ç”¨ä¾‹**:
```python
def test_pbo_not_constant():
    """PBOä¸åº”æ’ç­‰äº0.533"""
    detector = OverfittingDetector({})
    
    # æ„é€ é«˜è¿‡æ‹Ÿåˆæ•°æ®ï¼ˆISå¥½ï¼ŒOOSå·®ï¼‰
    high_overfit_results = [
        {'is_auc': 0.7, 'oos_auc': 0.5},
        {'is_auc': 0.65, 'oos_auc': 0.52},
        {'is_auc': 0.68, 'oos_auc': 0.48},
    ]
    pbo_high = detector.calculate_pbo(high_overfit_results)
    
    # æ„é€ ä½è¿‡æ‹Ÿåˆæ•°æ®ï¼ˆISå’ŒOOSæ¥è¿‘ï¼‰
    low_overfit_results = [
        {'is_auc': 0.6, 'oos_auc': 0.58},
        {'is_auc': 0.59, 'oos_auc': 0.57},
        {'is_auc': 0.61, 'oos_auc': 0.59},
    ]
    pbo_low = detector.calculate_pbo(low_overfit_results)
    
    # é«˜è¿‡æ‹Ÿåˆåº”æ¯”ä½è¿‡æ‹Ÿåˆæœ‰æ›´é«˜çš„PBO
    assert pbo_high > pbo_low, f"PBO should vary: {pbo_high} vs {pbo_low}"
    assert pbo_high != 0.533, "PBO should not be constant 0.533"
```

---

### ğŸ”´ C-02: MetaTrainerç«¯åˆ°ç«¯ä¸å¯è¿è¡Œ

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/meta_trainer.py`
- **ç—‡çŠ¶**: (a) FracDiffåˆ—ä¸å­˜åœ¨ (b) find_min_d_stationaryæœªè°ƒç”¨ (c) åªæœ‰5/15è·¯å¾„æœ‰æ•ˆ
- **å½±å“**: æ— æ³•å®Œæˆå®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹

**é—®é¢˜A: FracDiffåˆ—ä¸å­˜åœ¨**

**é—®é¢˜ä»£ç ** (`meta_trainer.py` ç¬¬235-245è¡Œ):
```python
# âŒ é”™è¯¯ï¼šåªæ˜¯æ£€æŸ¥ï¼Œæ²¡æœ‰è®¡ç®—
optimal_d = 0.5
frac_col = f'fracdiff_{int(optimal_d*10)}'

if frac_col not in features:
    current_features = features + [frac_col]  # åŠ å…¥ç‰¹å¾åˆ—è¡¨
else:
    current_features = features

# âŒ ä½† DataFrame ä¸­æ ¹æœ¬æ²¡æœ‰è¿™ä¸ªåˆ—ï¼
result = self._train_cpcv_fold(train_df, test_df, current_features)  # ä¼šæŠ¥ KeyError
```

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
def _train_cpcv_fold(
    self,
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    features: List[str],
    price_col: str = 'adj_close'
) -> Dict[str, Any]:
    """è®­ç»ƒå•ä¸ª CPCV foldï¼ŒåŒ…å« FracDiff ç‰¹å¾è®¡ç®—"""
    from src.features.fracdiff import find_min_d_stationary, fracdiff_fixed_window
    
    # Step 1: åœ¨è®­ç»ƒé›†ä¸Šæ‰¾æœ€ä¼˜ d
    optimal_d = find_min_d_stationary(
        train_df[price_col],
        d_values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5],
        adf_pvalue_threshold=0.05
    )
    
    logger.info(f"  Optimal d={optimal_d:.2f}")
    
    # Step 2: è®¡ç®— FracDiff ç‰¹å¾ï¼ˆtrain + testï¼‰
    window = 100
    train_df = train_df.copy()
    test_df = test_df.copy()
    
    train_df['fracdiff'] = fracdiff_fixed_window(
        train_df[price_col].values, optimal_d, window
    )
    test_df['fracdiff'] = fracdiff_fixed_window(
        test_df[price_col].values, optimal_d, window
    )
    
    # Step 3: æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
    current_features = features + ['fracdiff']
    
    # Step 4: å»é™¤ NaNï¼ˆFracDiff burn-in periodï¼‰
    train_df = train_df.dropna(subset=['fracdiff'])
    test_df = test_df.dropna(subset=['fracdiff'])
    
    # Step 5: è®­ç»ƒ LightGBM
    X_train = train_df[current_features]
    y_train = train_df[target_col]
    X_test = test_df[current_features]
    y_test = test_df[target_col]
    
    # ... åç»­è®­ç»ƒé€»è¾‘
```

**é—®é¢˜B: find_min_d_stationaryæœªè°ƒç”¨**

**ç°çŠ¶**: ä»£ç ç¡¬ç¼–ç  `optimal_d = 0.5`ï¼Œæœªä½¿ç”¨ ADF æ£€éªŒæ‰¾æœ€ä¼˜å€¼

**æ•´æ”¹æ–¹æ¡ˆ**: è§ä¸Šæ–¹ä»£ç ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè°ƒç”¨ `find_min_d_stationary()`

**é—®é¢˜C: ç”Ÿäº§é…ç½®å¯¼è‡´æ•°æ®ä¸è¶³**

**é…ç½®é—®é¢˜** (`config/training.yaml`):
```yaml
cpcv:
  n_splits: 6
  min_data_days: 630  # çº¦2.5å¹´æ•°æ®
```

**å½±å“**:
- 630å¤© / 6 folds = 105å¤©/fold
- purge_window=10, embargo_window=40 â†’ gap=50å¤©
- æœ‰æ•ˆè®­ç»ƒæ•°æ® = 105 - 50 = 55å¤© < 200å¤©é˜ˆå€¼
- **ç»“æœ**: åªæœ‰5/15è·¯å¾„æ»¡è¶³æœ€å°æ•°æ®è¦æ±‚

**æ•´æ”¹æ–¹æ¡ˆ**ï¼ˆä¸‰é€‰ä¸€ï¼‰:

**æ–¹æ¡ˆA: é™ä½min_data_daysï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰**
```yaml
cpcv:
  min_data_days: 450  # 450/6=75 > gap(50)ï¼Œæœ‰æ•ˆ25å¤©
```

**æ–¹æ¡ˆB: å‡å°‘n_splitsï¼ˆæ¨èï¼‰**
```yaml
cpcv:
  n_splits: 5  # C(5,2)=10æ¡è·¯å¾„ï¼ˆå‡å°‘ä½†è¶³å¤Ÿï¼‰
  min_data_days: 500  # 500/5=100 > gap(50)ï¼Œæœ‰æ•ˆ50å¤©
```

**æ–¹æ¡ˆC: å‡å°‘purge/embargoçª—å£**
```yaml
cpcv:
  purge_window: 5   # å‡å°‘åˆ°5å¤©
  embargo_window: 30  # å‡å°‘åˆ°30å¤©
  # gap = 35å¤©ï¼Œæ›´å¤šæœ‰æ•ˆè®­ç»ƒæ•°æ®
```

**æ¨è**: æ–¹æ¡ˆBï¼ˆå¹³è¡¡è·¯å¾„æ•°é‡å’Œæ•°æ®è´¨é‡ï¼‰

**éªŒæ”¶æ ‡å‡†**:
1. âœ… FracDiffç‰¹å¾æ­£ç¡®è®¡ç®—å¹¶æ·»åŠ åˆ°DataFrame
2. âœ… æ¯ä¸ªfoldè°ƒç”¨find_min_d_stationary()æ‰¾æœ€ä¼˜d
3. âœ… æ‰€æœ‰15æ¡è·¯å¾„ï¼ˆæˆ–10æ¡ï¼Œå¦‚æœç”¨æ–¹æ¡ˆBï¼‰æœ‰è¶³å¤Ÿè®­ç»ƒæ•°æ®
4. âœ… ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹æ— é”™è¯¯è¿è¡Œ
5. âœ… è¾“å‡ºåŒ…å«optimal_då€¼

**æµ‹è¯•ç”¨ä¾‹**:
```python
def test_meta_trainer_end_to_end():
    """ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•"""
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'symbol': ['AAPL'] * 1000,
        'date': pd.date_range('2020-01-01', periods=1000),
        'adj_close': 100 + np.random.randn(1000).cumsum(),
        'label': np.random.choice([-1, 1], 1000),
        'feature1': np.random.randn(1000),
    })
    
    trainer = MetaTrainer("config/training.yaml")
    base_model = BaseModelSMA(fast_window=20, slow_window=60)
    
    # åº”è¯¥èƒ½å®Œæˆè®­ç»ƒï¼Œä¸æŠ›å‡ºå¼‚å¸¸
    results = trainer.train(df, base_model, features=['feature1'])
    
    assert 'n_paths' in results
    assert results['n_paths'] >= 10  # è‡³å°‘10æ¡è·¯å¾„
    assert all('optimal_d' in r for r in results['paths'])  # æ¯æ¡è·¯å¾„éƒ½æœ‰optimal_d
```

---

### ğŸ”´ C-03: Sample Weightsæœªä¼ å…¥LightGBM

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/meta_trainer.py` ç¬¬199è¡Œ
- **ç—‡çŠ¶**: `lgb.Dataset` åˆ›å»ºæ—¶æœªä¼ å…¥ `weight` å‚æ•°
- **å½±å“**: æ ·æœ¬æƒé‡é…ç½®æ— æ•ˆï¼Œæ¨¡å‹è®­ç»ƒä¸è€ƒè™‘uniquenessæƒé‡

**é—®é¢˜ä»£ç **:
```python
# âŒ é”™è¯¯ï¼šæœªä¼ å…¥weightå‚æ•°
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
```

**é…ç½®å­˜åœ¨ä½†æœªä½¿ç”¨** (`config/training.yaml`):
```yaml
sample_weights:
  method: uniqueness
  min_weight: 0.01
  max_weight: 10.0
```

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
def _train_cpcv_fold(
    self,
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    features: List[str],
    target_col: str = 'meta_label'
) -> Dict[str, Any]:
    """è®­ç»ƒå•ä¸ª CPCV foldï¼ŒåŒ…å«æ ·æœ¬æƒé‡"""
    
    # Step 1: å‡†å¤‡æ•°æ®
    X_train = train_df[features]
    y_train = train_df[target_col]
    X_test = test_df[features]
    y_test = test_df[target_col]
    
    # Step 2: è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆåŸºäº uniquenessï¼‰
    train_weights = self._calculate_sample_weights(train_df)
    test_weights = self._calculate_sample_weights(test_df)
    
    # Step 3: åˆ›å»ºå¸¦æƒé‡çš„æ•°æ®é›†
    train_data = lgb.Dataset(
        X_train, 
        label=y_train,
        weight=train_weights  # âœ… ä¼ å…¥æƒé‡
    )
    valid_data = lgb.Dataset(
        X_test, 
        label=y_test, 
        reference=train_data,
        weight=test_weights  # âœ… ä¼ å…¥æƒé‡
    )
    
    # Step 4: è®­ç»ƒ
    model = lgb.train(
        self.lgb_params,
        train_data,
        num_boost_round=self.n_estimators,
        valid_sets=[valid_data],
        valid_names=['valid'],
        callbacks=[lgb.early_stopping(self.early_stopping_rounds, verbose=False)]
    )
    
    # ... åç»­é€»è¾‘

def _calculate_sample_weights(self, df: pd.DataFrame) -> np.ndarray:
    """
    è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆåŸºäº uniquenessï¼‰
    
    æ ¹æ® AFML Ch4ï¼Œæ ·æœ¬æƒé‡åº”åŸºäºï¼š
    1. Uniqueness: æ ·æœ¬çš„ç‹¬ç«‹ç¨‹åº¦
    2. Return: æ ·æœ¬çš„æ”¶ç›Šè´¡çŒ®ï¼ˆå¯é€‰ï¼‰
    """
    weight_config = self.config.get('sample_weights', {})
    method = weight_config.get('method', 'uniqueness')
    
    if method == 'uniqueness':
        # ä½¿ç”¨ uniqueness åˆ—ï¼ˆåº”ç”± Phase B ç”Ÿæˆï¼‰
        if 'uniqueness' in df.columns:
            weights = df['uniqueness'].values
        else:
            # Fallback: å‡åŒ€æƒé‡
            weights = np.ones(len(df))
    elif method == 'equal':
        weights = np.ones(len(df))
    else:
        weights = np.ones(len(df))
    
    # åº”ç”¨ min/max é™åˆ¶
    min_weight = weight_config.get('min_weight', 0.01)
    max_weight = weight_config.get('max_weight', 10.0)
    weights = np.clip(weights, min_weight, max_weight)
    
    # å½’ä¸€åŒ–ï¼ˆä¿æŒå‡å€¼=1ï¼‰
    weights = weights / weights.mean()
    
    return weights
```

**éªŒæ”¶æ ‡å‡†**:
1. âœ… LightGBM Dataset åŒ…å« weight å‚æ•°
2. âœ… æ ·æœ¬æƒé‡åŸºäº uniqueness è®¡ç®—
3. âœ… æƒé‡å€¼åœ¨ [min_weight, max_weight] èŒƒå›´å†…
4. âœ… å•å…ƒæµ‹è¯•éªŒè¯æƒé‡ä¼ é€’
5. âœ… è®­ç»ƒæ—¥å¿—åŒ…å«æƒé‡ç»Ÿè®¡ä¿¡æ¯

**æµ‹è¯•ç”¨ä¾‹**:
```python
def test_sample_weights_passed_to_lgb():
    """éªŒè¯æ ·æœ¬æƒé‡ä¼ å…¥LightGBM"""
    import lightgbm as lgb
    
    # Mock æ•°æ®
    train_df = pd.DataFrame({
        'feature1': [1, 2, 3, 4, 5],
        'meta_label': [0, 1, 0, 1, 0],
        'uniqueness': [0.5, 0.8, 0.6, 0.9, 0.4]
    })
    
    trainer = MetaTrainer("config/training.yaml")
    weights = trainer._calculate_sample_weights(train_df)
    
    # éªŒè¯æƒé‡éå‡åŒ€
    assert not np.allclose(weights, weights[0]), "Weights should not be uniform"
    
    # éªŒè¯æƒé‡èŒƒå›´
    config = trainer.config.get('sample_weights', {})
    min_w = config.get('min_weight', 0.01)
    max_w = config.get('max_weight', 10.0)
    assert weights.min() >= min_w
    assert weights.max() <= max_w
```

---

## ä¸‰ã€HIGHçº§é—®é¢˜æ•´æ”¹æ–¹æ¡ˆ

### âš ï¸ H-01: DSRæ•°å€¼ä¸ç¨³å®š

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/overfitting.py` ç¬¬133-165è¡Œ
- **ç—‡çŠ¶**: å½“ std=0 æˆ– n<2 æ—¶è¿”å›0ï¼Œæœªæ ‡è®°ä¸ºæ— æ•ˆ

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
def calculate_deflated_sharpe(self, path_results: List[Dict]) -> Tuple[float, bool]:
    """
    è®¡ç®— DSR z-scoreï¼Œè¿”å› (å€¼, æ˜¯å¦æœ‰æ•ˆ)
    """
    metrics = [r.get('accuracy', r.get('auc', 0.5)) for r in path_results]
    
    if len(metrics) < 2:
        logger.warn("deflated_sharpe_insufficient_data", {"n_paths": len(metrics)})
        return 0.0, False  # âœ… æ ‡è®°ä¸ºæ— æ•ˆ
    
    mean_sr = np.mean(metrics)
    std_sr = np.std(metrics, ddof=1)
    n = len(metrics)
    
    if std_sr == 0 or n < 2:
        logger.warn("deflated_sharpe_zero_variance", {"std": std_sr, "n": n})
        return 0.0, False  # âœ… æ ‡è®°ä¸ºæ— æ•ˆ
    
    se_sr = std_sr / np.sqrt(n)
    baseline = 0.5
    dsr = (mean_sr - baseline) / se_sr
    
    return float(dsr), True  # âœ… æœ‰æ•ˆ
```

---

### âš ï¸ H-02: CPCVä½¿ç”¨æ—¥å†æ—¥è€Œéäº¤æ˜“æ—¥

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/purged_kfold.py`
- **ç—‡çŠ¶**: purge_window=10 ä½¿ç”¨æ—¥å†æ—¥ï¼Œéäº¤æ˜“æ—¥

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
def _apply_purge(self, df: pd.DataFrame, ...):
    """åº”ç”¨ purgeï¼Œä½¿ç”¨äº¤æ˜“æ—¥è€Œéæ—¥å†æ—¥"""
    
    # âœ… ä½¿ç”¨ pd.tseries.offsets.BDay (business day)
    from pandas.tseries.offsets import BDay
    
    purge_start = test_start - BDay(self.purge_window)
    purge_end = test_start
    
    # ... åç»­é€»è¾‘
```

---

### âš ï¸ H-03: BDay(1)è½å‡æ—¥é—®é¢˜

**é—®é¢˜æè¿°**:
- **ç—‡çŠ¶**: `BDay(1)` å¯èƒ½è½åœ¨å‡æ—¥ï¼ˆå¦‚ä¸­å›½æ˜¥èŠ‚ï¼‰

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
from pandas.tseries.holiday import USFederalHolidayCalendar

# ä½¿ç”¨äº¤æ˜“æ—¥å†
us_calendar = USFederalHolidayCalendar()
business_day = pd.tseries.offsets.CustomBusinessDay(calendar=us_calendar)

purge_start = test_start - business_day(self.purge_window)
```

---

### âš ï¸ H-05: assertå¯ç»•è¿‡

**é—®é¢˜æè¿°**:
- **æ–‡ä»¶**: `src/models/meta_trainer.py` ç¬¬89-96è¡Œ
- **ç—‡çŠ¶**: ç”Ÿäº§ç¯å¢ƒå¯ç”¨ `-O` æ ‡å¿—ç»•è¿‡ assert

**æ•´æ”¹æ–¹æ¡ˆ**:
```python
# âŒ é”™è¯¯ï¼šassert å¯è¢« -O ç»•è¿‡
assert max_depth <= 3, f"OR5: max_depth must be <= 3"

# âœ… æ­£ç¡®ï¼šä½¿ç”¨æ˜¾å¼æ£€æŸ¥
if max_depth > 3:
    raise ValueError(f"OR5 VIOLATION: max_depth={max_depth} > 3")
if num_leaves > 7:
    raise ValueError(f"OR5 VIOLATION: num_leaves={num_leaves} > 7")
if min_data_in_leaf < 100:
    raise ValueError(f"OR5 VIOLATION: min_data_in_leaf={min_data_in_leaf} < 100")
```

---

## å››ã€æ‰§è¡Œæ—¶é—´è¡¨

### Week 1 (2026-03-02 ~ 2026-03-06)

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æ—¶ |
|------|------|--------|------|
| Day 1 ä¸Šåˆ | C-01: PBOä¿®å¤ | æå¾—å‹¤ | 2h |
| Day 1 ä¸‹åˆ | C-01: æµ‹è¯•éªŒè¯ | æå¾—å‹¤ | 2h |
| Day 2 å…¨å¤© | C-02: MetaTrainerä¿®å¤ï¼ˆFracDiff+find_min_dï¼‰ | æå¾—å‹¤ | 6h |
| Day 3 ä¸Šåˆ | C-02: é…ç½®è°ƒæ•´ï¼ˆn_splits/min_data_daysï¼‰ | æå¾—å‹¤ | 2h |
| Day 3 ä¸‹åˆ | C-03: Sample Weightsä¿®å¤ | æå¾—å‹¤ | 2h |

### Week 2 (2026-03-09 ~ 2026-03-13)

| æ—¥æœŸ | ä»»åŠ¡ | è´Ÿè´£äºº | å·¥æ—¶ |
|------|------|--------|------|
| Day 1 ä¸Šåˆ | H-01: DSRç¨³å®šæ€§ | æå¾—å‹¤ | 1h |
| Day 1 ä¸‹åˆ | H-02: CPCVäº¤æ˜“æ—¥ä¿®å¤ | æå¾—å‹¤ | 2h |
| Day 2 ä¸Šåˆ | H-03: BDayå‡æ—¥å¤„ç† | æå¾—å‹¤ | 2h |
| Day 2 ä¸‹åˆ | H-05: assertæ›¿æ¢ | æå¾—å‹¤ | 1h |
| Day 3 | é›†æˆæµ‹è¯• + æ–‡æ¡£æ›´æ–° | æå¾—å‹¤ | 4h |

---

## äº”ã€éªŒæ”¶æ£€æŸ¥æ¸…å•

### CRITICALé˜»å¡é¡¹éªŒæ”¶

- [ ] **C-01**: PBOå€¼ä¸å†æ’ç­‰äº0.533
  - [ ] å•å…ƒæµ‹è¯•é€šè¿‡ï¼š`test_pbo_not_constant()`
  - [ ] ä¸åŒæ•°æ®é›†äº§ç”Ÿä¸åŒPBO
  - [ ] æ–‡æ¡£æ›´æ–°

- [ ] **C-02**: MetaTrainerç«¯åˆ°ç«¯å¯è¿è¡Œ
  - [ ] FracDiffç‰¹å¾æ­£ç¡®è®¡ç®—
  - [ ] find_min_d_stationary() è¢«è°ƒç”¨
  - [ ] æ‰€æœ‰è·¯å¾„æœ‰è¶³å¤Ÿè®­ç»ƒæ•°æ®ï¼ˆ>= 10æ¡ï¼‰
  - [ ] é›†æˆæµ‹è¯•é€šè¿‡ï¼š`test_meta_trainer_end_to_end()`

- [ ] **C-03**: Sample Weightsæ­£ç¡®ä¼ é€’
  - [ ] lgb.Dataset åŒ…å« weight å‚æ•°
  - [ ] æƒé‡åŸºäº uniqueness è®¡ç®—
  - [ ] å•å…ƒæµ‹è¯•é€šè¿‡ï¼š`test_sample_weights_passed_to_lgb()`

### HIGHçº§é—®é¢˜éªŒæ”¶

- [ ] **H-01**: DSRè¿”å›æœ‰æ•ˆæ€§æ ‡è®°
- [ ] **H-02**: CPCVä½¿ç”¨ BDay è€Œéæ—¥å†æ—¥
- [ ] **H-03**: ä½¿ç”¨äº¤æ˜“æ—¥å†å¤„ç†å‡æ—¥
- [ ] **H-05**: assert æ›¿æ¢ä¸ºæ˜¾å¼ ValueError

### æœ€ç»ˆéªŒæ”¶

- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼š`pytest tests/ -v`
- [ ] ç«¯åˆ°ç«¯æµç¨‹æ— é”™è¯¯ï¼š`python run_pipeline.py --mode train`
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡ï¼ˆææˆè£å…¬å…¬å®¡æ‰¹ï¼‰
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ

---

## å…­ã€é£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| FracDiffè®¡ç®—è€—æ—¶ | è®­ç»ƒæ—¶é—´å¢åŠ  | é¢„è®¡ç®— + ç¼“å­˜ |
| é…ç½®è°ƒæ•´å½±å“ç°æœ‰æµ‹è¯• | æµ‹è¯•å¤±è´¥ | åŒæ­¥æ›´æ–°æµ‹è¯•å›ºä»¶ |
| äº¤æ˜“æ—¥å†æ•°æ®ç¼ºå¤± | BDayè®¡ç®—é”™è¯¯ | ä½¿ç”¨ pandas å†…ç½® USFederalHolidayCalendar |
| Sample weightsæç«¯å€¼ | æ¨¡å‹ä¸ç¨³å®š | clip(min_weight, max_weight) |

---

## ä¸ƒã€é™„ä»¶

### A. ä¿®å¤åä»£ç ç¤ºä¾‹

è§å„CRITICALé—®é¢˜çš„"æ•´æ”¹æ–¹æ¡ˆ"éƒ¨åˆ†ã€‚

### B. æµ‹è¯•è®¡åˆ’

1. **å•å…ƒæµ‹è¯•**: æ¯ä¸ªä¿®å¤é¡¹å¯¹åº”ç‹¬ç«‹æµ‹è¯•
2. **é›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
3. **å›å½’æµ‹è¯•**: ç¡®ä¿åŸæœ‰åŠŸèƒ½ä¸å—å½±å“

### C. å›æ»šæ–¹æ¡ˆ

å¦‚ä¿®å¤å¼•å…¥æ–°é—®é¢˜ï¼š
1. Git revert å¯¹åº” commit
2. æ¢å¤åŸé…ç½®æ–‡ä»¶
3. é‡æ–°è¯„ä¼°ä¿®å¤æ–¹æ¡ˆ

---

**ç¼–åˆ¶å®Œæˆæ—¥æœŸ**: 2026-02-28  
**ä¸‹æ¬¡å®¡è®¡æ—¥æœŸ**: 2026-03-16

---

*å¼ å¾—åŠŸè°¨å‘ˆ*  
*å…«å“é¢†ä¾*
